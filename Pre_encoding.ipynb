{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1243ffe7-e500-4f25-9ead-e133e70347fb",
   "metadata": {},
   "source": [
    "##### Pre-encoding all parts from the warehouse into feature vector form, for better storage and real-time access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab34d8c5-2a55-47f9-877f-b4b613537e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import sys\n",
    "sys.path.append(\"/home/beast/Desktop/vlassis/retrieval2/experiments\")\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "from scripts.dataset import Items3Dataset, Warehouse4Dataset, warehouse4_collate_fn\n",
    "from scripts.visualization import quick_vis, quick_vis_with_parts, quick_vis_many\n",
    "from scripts.visualization import plot_histogram, visualize_distribution_over_time, rotation_matrix_from_vectors2\n",
    "from scripts.visualization import quick_vis_pretty, quick_vis_with_parts_pretty\n",
    "from scripts.model import *\n",
    "from scripts.logger import LivePlot\n",
    "from scripts.metrics import AccuracyMultiClass\n",
    "from scripts.utils import map_labels, generate_label_map, normalize_parts, normalize_parts_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd77a83-5f17-40e0-811b-1efc1c70084e",
   "metadata": {},
   "source": [
    "##### Choose and instantiate the model, load the appropriate checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4167f3f9-3f4c-4936-a3f2-eaddeaaa6e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [0, 1, 3]\n",
    "\n",
    "#Load the model we want\n",
    "model = PartFinderPipeline2(in_channels = 3, out_channels = 384,\n",
    "                           num_classes = len(categories),\n",
    "                           num_attention_blocks = 3,\n",
    "                           pos_emb_dim = 3,\n",
    "                           pool_method = \"cls_token_pool\"\n",
    "                          ).cuda()\n",
    "\n",
    "#CLS_allcats_T7_partnet_pointnetnew_batchless.pt\n",
    "model.load_state_dict(torch.load(\"/home/beast/Desktop/vlassis/retrieval2/checkpoints/CLS_allcats_T7_shapenet.pt\"))\n",
    "model.eval()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1509407c-a3e6-4d04-a37a-848a0d2d15a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing warehouse dataset\n",
      "Warehouse dataset initialization complete (t = 0.03170418739318848)\n"
     ]
    }
   ],
   "source": [
    "#Encoding ALL samples, not just the specified categories. Works by design of the warehouse dataset\n",
    "data_path = '/home/beast/Desktop/vlassis/retrieval2/experiments/data/vectors_warehouse_shapenet'\n",
    "categories = [0, 1, 3]\n",
    "warehouse = Warehouse4Dataset(cat = None, path=data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56c6c6a-773c-4816-a24d-e290ea0c78c2",
   "metadata": {},
   "source": [
    "##### Encode every sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5ed886d-304c-445e-b060-fcbbbab3bc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62120it [02:26, 424.78it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i, (sample, label, part_label, vec) in tqdm(enumerate(warehouse)):\n",
    "\n",
    "        #transferring to gpu\n",
    "        sample = torch.from_numpy(sample).cuda().float()\n",
    "\n",
    "        #normalizing translation\n",
    "        sample = sample - sample.mean(dim=0)\n",
    "\n",
    "        #DO NOT ROTATE, THE WAREHOUSE DATASET IS PREROTATED\n",
    "        \n",
    "        ####IF YOU CHANGE THE PARTS TO BE FED INTO THE MODEL HAVING NORMALIZED THEIR SCALE\n",
    "        #DO THE SAME HERE\n",
    "        \n",
    "        #creating a dummy PID\n",
    "        pid = torch.zeros(sample.shape[0]).cuda().long()\n",
    "\n",
    "        #running the model - features: 1 x F\n",
    "        features = model.forward_encoder(sample, pid)\n",
    "        \n",
    "        #adding the encoding to the warehouse data files (Warehouse4 only)\n",
    "        warehouse.add_encoding(i, features.squeeze().cpu())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
